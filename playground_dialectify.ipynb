{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the sql to encode and decode only some parts of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name, age, email FROM users\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "# Define the SQL statement\n",
    "sql_statement = \"SELECT name, age, email FROM users\"\n",
    "\n",
    "# Parse the SQL statement using sqlparse\n",
    "parsed_statement = sqlparse.parse(sql_statement)[0]\n",
    "\n",
    "print(parsed_statement)\n",
    "\n",
    "# Extract the field names from the SELECT statement\n",
    "# fields = [str(token) for token in parsed_statement.tokens if token.ttype is sqlparse.tokens.Name]\n",
    "\n",
    "# # Print the field names\n",
    "# print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'cli',\n",
       " 'engine',\n",
       " 'exceptions',\n",
       " 'filters',\n",
       " 'format',\n",
       " 'formatter',\n",
       " 'keywords',\n",
       " 'lexer',\n",
       " 'parse',\n",
       " 'parsestream',\n",
       " 'split',\n",
       " 'sql',\n",
       " 'tokens',\n",
       " 'utils']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "dir(sqlparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select * from foo;', 'select * from bar;']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'select * from foo; select * from bar;'\n",
    "statements = sqlparse.split(raw)\n",
    "statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT *\n",
      "FROM foo;\n"
     ]
    }
   ],
   "source": [
    "# Format the first statement and print it out:\n",
    "first = statements[0]\n",
    "print(sqlparse.format(first, reindent=True, keyword_case='upper'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DML 'select' at 0x118EAAE60>,\n",
       " <Whitespace ' ' at 0x11990D900>,\n",
       " <IdentifierList 'order....' at 0x119909F50>,\n",
       " <Whitespace ' ' at 0x11990F880>,\n",
       " <Keyword 'from' at 0x11990F8E0>,\n",
       " <Whitespace ' ' at 0x11990F940>,\n",
       " <Identifier 'orders...' at 0x119909CB0>,\n",
       " <Whitespace ' ' at 0x11990FAC0>,\n",
       " <Keyword 'join' at 0x11990FB20>,\n",
       " <Whitespace ' ' at 0x11990FB80>,\n",
       " <Identifier 'revenu...' at 0x119909D90>,\n",
       " <Whitespace ' ' at 0x11990FD00>,\n",
       " <Keyword 'on' at 0x11990FD60>,\n",
       " <Whitespace ' ' at 0x11990FDC0>,\n",
       " <Comparison 'r.orde...' at 0x119909EE0>,\n",
       " <Whitespace ' ' at 0x119934220>,\n",
       " <Keyword 'order ...' at 0x119934280>,\n",
       " <Whitespace ' ' at 0x1199342E0>,\n",
       " <Identifier 'order....' at 0x119909E70>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = sqlparse.parse('select order.orderid, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc')[0]\n",
    "parsed.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: select\n",
      "Token 1:  \n",
      "Token 2: order.orderid, order.order_name, revenue.order_id, r.product_name, revenue.cost\n",
      "Token 3:  \n",
      "Token 4: from\n",
      "Token 5:  \n",
      "Token 6: orders o\n",
      "Token 7:  \n",
      "Token 8: join\n",
      "Token 9:  \n",
      "Token 10: revenue r\n",
      "Token 11:  \n",
      "Token 12: on\n",
      "Token 13:  \n",
      "Token 14: r.order_id  = r.order_id\n",
      "Token 15:  \n",
      "Token 16: order by\n",
      "Token 17:  \n",
      "Token 18: order.orderid desc\n"
     ]
    }
   ],
   "source": [
    "# Parsing a SQL statement:\n",
    "parsed = sqlparse.parse('select order.orderid, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc')[0]\n",
    "parsed_tokens = parsed.tokens\n",
    "for i, token in enumerate(parsed_tokens):\n",
    "    print(f\"Token {i}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 14\n",
      "IdentifierList: 1\n",
      "Identifier: 3\n",
      "Comparison: 1\n"
     ]
    }
   ],
   "source": [
    "parsed_tokens = parsed.tokens\n",
    "token_counts = {}\n",
    "\n",
    "for i, token in enumerate(parsed_tokens):\n",
    "    tok_type = type(token).__name__\n",
    "    if tok_type in token_counts:\n",
    "        token_counts[tok_type] += 1\n",
    "    else:\n",
    "        token_counts[tok_type] = 1\n",
    "\n",
    "for tok_type, count in token_counts.items():\n",
    "    print(f\"{tok_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite above function but return a list of fields per type in a data table\n",
    "def get_fields(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    token_counts = {}\n",
    "    for i, token in enumerate(parsed_tokens):\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type in token_counts:\n",
    "            token_counts[tok_type].append(token)\n",
    "        else:\n",
    "            token_counts[tok_type] = [token]\n",
    "    #return a dictionary of types and their corresponding tokens in a data table\n",
    "    return token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ['SELECT', ' ', ' ', 'FROM', ' ', ' ']\n",
      "IdentifierList: ['first_name, last_name']\n",
      "Identifier: ['employees']\n",
      "Where: ['WHERE salary > 50000']\n"
     ]
    }
   ],
   "source": [
    "from sqlparse import parse\n",
    "\n",
    "# Example SQL query\n",
    "#query = \"select order.order_id, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc\"\n",
    "query = \"SELECT first_name, last_name FROM employees WHERE salary > 50000\"\n",
    "\n",
    "# Parse the query into tokens\n",
    "parsed = parse(query)[0]\n",
    "\n",
    "# Get the fields in the parsed query\n",
    "fields = get_fields(parsed)\n",
    "\n",
    "# Print out the types and tokens in the data table\n",
    "for tok_type, tokens in fields.items():\n",
    "    print(f\"{tok_type}: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders o', 'revenue r', 'order', 'orderid desc']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "def get_identifiers(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    identifiers = []\n",
    "    for token in parsed_tokens:\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type == 'Whitespace':\n",
    "            continue\n",
    "        elif tok_type == 'Keyword':\n",
    "            continue\n",
    "        elif tok_type == 'Identifier':\n",
    "            identifier = str(token)\n",
    "            if '.' in identifier:\n",
    "                identifiers.extend(identifier.split('.'))\n",
    "            else:\n",
    "                identifiers.append(identifier)\n",
    "        elif tok_type == 'Name':\n",
    "            identifiers.append(str(token))\n",
    "    return identifiers\n",
    "\n",
    "#sql = \"SELECT first_name, last_name FROM employees WHERE department='Sales'\"\n",
    "sql = \"select order.order_id, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc\"\n",
    "\n",
    "parsed = sqlparse.parse(sql)[0]\n",
    "identifiers = get_identifiers(parsed)\n",
    "print(identifiers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders o', 'revenue r', 'order', 'orderid desc']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "def get_identifiers(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    identifiers = []\n",
    "    for token in parsed_tokens:\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type == 'Whitespace':\n",
    "            continue\n",
    "        elif tok_type == 'Keyword':\n",
    "            continue\n",
    "        elif tok_type == 'Identifier':\n",
    "            identifier = str(token)\n",
    "            if '.' in identifier:\n",
    "                identifiers.extend(identifier.split('.'))\n",
    "            else:\n",
    "                identifiers.append(identifier)\n",
    "        elif tok_type == 'Name':\n",
    "            identifiers.append(str(token))\n",
    "    return identifiers\n",
    "\n",
    "#sql = \"SELECT first_name, last_name FROM employees WHERE department='Sales'\"\n",
    "sql = \"select order.order_id, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc\"\n",
    "\n",
    "parsed = sqlparse.parse(sql)[0]\n",
    "identifiers = get_identifiers(parsed)\n",
    "print(identifiers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders o', 'revenue r', 'order', 'orderid desc']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "def get_identifiers(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    identifiers = []\n",
    "    for token in parsed_tokens:\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type == 'Whitespace':\n",
    "            continue\n",
    "        elif tok_type == 'Keyword':\n",
    "            continue\n",
    "        elif tok_type == 'Identifier':\n",
    "            identifier = str(token)\n",
    "            if '.' in identifier:\n",
    "                identifiers.extend(identifier.split('.'))\n",
    "            else:\n",
    "                identifiers.append(identifier)\n",
    "        elif tok_type == 'Name':\n",
    "            identifiers.append(str(token))\n",
    "    return identifiers\n",
    "\n",
    "#sql = \"SELECT first_name, last_name FROM employees WHERE department='Sales'\"\n",
    "sql = \"select order.order_id, order.order_name, revenue.order_id, r.product_name, revenue.cost from orders o join revenue r on r.order_id  = r.order_id order by order.orderid desc\"\n",
    "\n",
    "parsed = sqlparse.parse(sql)[0]\n",
    "identifiers = get_identifiers(parsed)\n",
    "print(identifiers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['column1', 'column2', 'table1']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "sql = \"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\"\n",
    "parsed = sqlparse.parse(sql)[0]\n",
    "\n",
    "identifiers = get_identifiers(parsed)\n",
    "print(identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier_list(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    identifier_list = []\n",
    "    for token in parsed_tokens:\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type == 'IdentifierList':\n",
    "            for identifier in token.get_identifiers():\n",
    "                if not isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    continue\n",
    "                identifier_name = identifier.get_name()\n",
    "                if '.' in identifier_name:\n",
    "                    identifier_name = identifier_name.split('.')[1]\n",
    "                identifier_list.append(identifier_name)\n",
    "    return identifier_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['column1', 'column2']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "sql = \"SELECT column1, column2 FROM table1 WHERE column3 = 'value'\"\n",
    "parsed = sqlparse.parse(sql)[0]\n",
    "\n",
    "identifier_list = get_identifier_list(parsed)\n",
    "print(identifier_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tok_types(parsed):\n",
    "    parsed_tokens = parsed.tokens\n",
    "    tok_types = []\n",
    "    for token in parsed_tokens:\n",
    "        tok_type = type(token).__name__\n",
    "        if tok_type in ['Identifier', 'Comparison', 'IdentifierList', 'Where']:\n",
    "            tok_types.append(tok_type)\n",
    "    return tok_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdentifierList', 'Identifier', 'Where']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "query = \"SELECT first_name, last_name FROM employees WHERE salary > 50000\"\n",
    "parsed_query = sqlparse.parse(query)[0]\n",
    "tok_types = get_tok_types(parsed_query)\n",
    "print(tok_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Identifier 'elect ...' at 0x119F04740>,\n",
       " <Whitespace ' ' at 0x11AB80280>,\n",
       " <Integer '10' at 0x11AB801C0>,\n",
       " <Whitespace ' ' at 0x11AB825C0>,\n",
       " <IdentifierList 'getdat...' at 0x11AADB4C0>,\n",
       " <Whitespace ' ' at 0x11AB81900>,\n",
       " <Keyword 'from' at 0x11AB827A0>,\n",
       " <Whitespace ' ' at 0x11AB818A0>,\n",
       " <Identifier 'orders...' at 0x119F04200>,\n",
       " <Whitespace ' ' at 0x11AAB41C0>,\n",
       " <Keyword 'join' at 0x11AAB4040>,\n",
       " <Whitespace ' ' at 0x11AAB46A0>,\n",
       " <Identifier 'revenu...' at 0x119F044A0>,\n",
       " <Whitespace ' ' at 0x11AAB4DC0>,\n",
       " <Keyword 'on' at 0x11AAB4D60>,\n",
       " <Whitespace ' ' at 0x11AAB50C0>,\n",
       " <Comparison 'o.orde...' at 0x11AADB680>]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "query = \"Select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id\"\n",
    "parsed_query = sqlparse.parse(query)[0]\n",
    "parsed_query.tokens\n",
    "#print(parsed_query.tokens)\n",
    "# tok_types = get_identifiers(query)\n",
    "# print(tok_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n",
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "lst = ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# Mask string literals in the list\n",
    "masked_lst = mask_string_literals(lst)\n",
    "print(masked_lst)  # Output: [\"'first_name'\", 'employees', \"'last_name'\", \"'salary'\", \"'department'\"]\n",
    "\n",
    "# Unmask string literals in the list\n",
    "unmasked_lst = unmask_string_literals(masked_lst)\n",
    "print(unmasked_lst)  # Output: ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# # Apply mask to SQL code\n",
    "# sql = \"SELECT first_name FROM employees WHERE department = 'Sales'\"\n",
    "# masked_sql = apply_mask_to_sql(sql)\n",
    "# print(masked_sql)  # Output: \"SELECT first_name FROM employees WHERE department = ''Sales''\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n",
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "lst = ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# Mask string literals in the list\n",
    "masked_lst = mask_string_literals(lst)\n",
    "print(masked_lst)  # Output: [\"'first_name'\", 'employees', \"'last_name'\", \"'salary'\", \"'department'\"]\n",
    "\n",
    "# Unmask string literals in the list\n",
    "unmasked_lst = unmask_string_literals(masked_lst)\n",
    "print(unmasked_lst)  # Output: ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# # Apply mask to SQL code\n",
    "# sql = \"SELECT first_name FROM employees WHERE department = 'Sales'\"\n",
    "# masked_sql = apply_mask_to_sql(sql)\n",
    "# print(masked_sql)  # Output: \"SELECT first_name FROM employees WHERE department = ''Sales''\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n",
      "['first_name', 'employees', 'last_name', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "lst = ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# Mask string literals in the list\n",
    "masked_lst = mask_string_literals(lst)\n",
    "print(masked_lst)  # Output: [\"'first_name'\", 'employees', \"'last_name'\", \"'salary'\", \"'department'\"]\n",
    "\n",
    "# Unmask string literals in the list\n",
    "unmasked_lst = unmask_string_literals(masked_lst)\n",
    "print(unmasked_lst)  # Output: ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# # Apply mask to SQL code\n",
    "# sql = \"SELECT first_name FROM employees WHERE department = 'Sales'\"\n",
    "# masked_sql = apply_mask_to_sql(sql)\n",
    "# print(masked_sql)  # Output: \"SELECT first_name FROM employees WHERE department = ''Sales''\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_number', 'o', None, 'sum', 'top', 'client_name', 'r', 'getdate']\n"
     ]
    }
   ],
   "source": [
    "#Alles zusammen - Working for parsing fields from a sql statement. Decide to do it modularly, not sure why but this works. That is why\n",
    "#works for where!!!\n",
    "import sqlparse\n",
    "\n",
    "def get_where_fields(query):\n",
    "    parsed_query = sqlparse.parse(query)[0]\n",
    "    where_clause = None\n",
    "    for token in parsed_query.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Where):\n",
    "            where_clause = token\n",
    "            break\n",
    "    if not where_clause:\n",
    "        return []\n",
    "    fields = []\n",
    "    for token in where_clause.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Comparison):\n",
    "            left = token.left\n",
    "            if isinstance(left, sqlparse.sql.Identifier):\n",
    "                fields.append(left.get_name())\n",
    "            elif isinstance(left, sqlparse.sql.Function):\n",
    "                fields.append(left.tokens[0].get_name())\n",
    "    return fields\n",
    "\n",
    "# Works for identifiers\n",
    "def get_identifiers(query):\n",
    "    parsed_tokens = sqlparse.parse(query)[0]\n",
    "    identifier_set = set()\n",
    "    for token in parsed_tokens:\n",
    "        # If the token is a function, skip it\n",
    "        if isinstance(token, sqlparse.sql.Function):\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                identifier_name = identifier.get_name()\n",
    "                if '.' in identifier_name:\n",
    "                    identifier_name = identifier_name.split('.')[1]\n",
    "                identifier_set.add(identifier_name)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            identifier_name = token.get_name()\n",
    "            if '.' in identifier_name:\n",
    "                identifier_name = identifier_name.split('.')[1]\n",
    "            identifier_set.add(identifier_name)\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            identifier_name = token.get_name()\n",
    "            identifier_set.add(identifier_name)\n",
    "        elif isinstance(token, sqlparse.sql.Where):\n",
    "            identifier_name = token.get_name()\n",
    "            identifier_set.add(identifier_name)\n",
    "        elif isinstance(token, sqlparse.sql.Function):\n",
    "            continue\n",
    "    return list(identifier_set)\n",
    "\n",
    "import sqlparse\n",
    "#query = \"SELECT first_name, last_name FROM employees WHERE salary > 50000 AND department = 'Sales'\"\n",
    "query =\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id\"\n",
    "identifiers = get_identifiers(query)\n",
    "# where_fields = get_where_fields(query)\n",
    "# list_of_fields = identifiers + where_fields\n",
    "# print(list_of_fields)\n",
    "print(identifiers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Finally working for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_number', 'revenue', 'order_id', 'orders', 'client_name']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "def get_identifiers(sql):\n",
    "    parsed_tokens = sqlparse.parse(sql)[0]\n",
    "    identifier_set = set()\n",
    "    \n",
    "    reserved_words = ['TOP', 'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY', 'HAVING', 'ORDER', 'ASC', 'DESC']\n",
    "\n",
    "    for token in parsed_tokens.tokens:\n",
    "        # If the token is a function, skip it\n",
    "        if isinstance(token, sqlparse.sql.Function) or token.value.upper() in reserved_words:\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.Function):\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    identifier_name = identifier.get_real_name()\n",
    "                    if '.' in identifier_name:\n",
    "                        identifier_name = identifier_name.split('.')[1]\n",
    "                    identifier_set.add(identifier_name)\n",
    "        # If the token is a comparison operator, get the column name\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            identifier_name = token.left.get_real_name()\n",
    "            if '.' in identifier_name:\n",
    "                identifier_name = identifier_name.split('.')[1]\n",
    "            identifier_set.add(identifier_name)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            identifier_name = token.get_real_name()\n",
    "            if '.' in identifier_name:\n",
    "                identifier_name = identifier_name.split('.')[1]\n",
    "            identifier_set.add(identifier_name)\n",
    "    return list(identifier_set)\n",
    "\n",
    "\n",
    "\n",
    "query =\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_order in\"\n",
    "identifiers = get_identifiers(query)\n",
    "print(identifiers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking and Demasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def masking(list_of_fields, sql):\n",
    "    \"\"\"\n",
    "    This function takes in a list of words and a SQL string as input and replaces the words in the SQL string with random words.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the mapping between original words and masked words\n",
    "    word_map = {}\n",
    "    \n",
    "    # Loop through each word in the list of words\n",
    "    for word in list_of_fields:\n",
    "        # Generate a random word to replace the original word\n",
    "        random_word = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=len(word)))\n",
    "        \n",
    "        # Add the mapping to the dictionary\n",
    "        word_map[word] = random_word\n",
    "        \n",
    "        # Replace the original word with the random word in the SQL string\n",
    "        sql = re.sub(r'\\b{}\\b'.format(word), random_word, sql)\n",
    "    \n",
    "    # Return the masked SQL string and the word map\n",
    "    return sql, word_map\n",
    "\n",
    "\n",
    "def demasking(word_map, sql):\n",
    "    \"\"\"\n",
    "    This function takes in a word map and a masked SQL string as input and replaces the masked words with their original words.\n",
    "    \"\"\"\n",
    "    # Loop through each key-value pair in the word map\n",
    "    for original_word, masked_word in word_map.items():\n",
    "        # Replace the masked word with the original word in the SQL string\n",
    "        sql_string = re.sub(r'\\b{}\\b'.format(masked_word), original_word, sql)\n",
    "    \n",
    "    # Return the demasked SQL string\n",
    "    return sql_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked SQL string: select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_order in\n",
      "Word map: {'first_name': 'shyuckvdgw', 'employees': 'uudyfsqyx', 'last_name': 'rvecpyxir', 'salary': 'yqrvxk', 'department': 'uwlebjbjak'}\n",
      "Demasked SQL string: select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_order in\n"
     ]
    }
   ],
   "source": [
    "# Define the list of words to mask\n",
    "words_to_mask = ['first_name', 'employees', 'last_name', 'salary', 'department']\n",
    "\n",
    "# Define the SQL string to mask\n",
    "sql = \"SELECT first_name, last_name FROM employees WHERE salary > 50000 AND department = 'Sales'\"\n",
    "\n",
    "# Mask the SQL string\n",
    "masked_sql_string, word_map = masking(words_to_mask, query)\n",
    "\n",
    "# Print the masked SQL string and the word map\n",
    "print(\"Masked SQL string:\", masked_sql_string)\n",
    "print(\"Word map:\", word_map)\n",
    "\n",
    "# Demask the SQL string\n",
    "demasked_sql_string = demasking(word_map, masked_sql_string)\n",
    "\n",
    "# Print the demasked SQL string\n",
    "print(\"Demasked SQL string:\", demasked_sql_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked SQL string: select ciw 10 ufbdnsy(), wjdonbpitghk, ndyykbwdxur, tcv(net_revenue) from orders x join revenue y on x.order_id = y.order_id\n",
      "Word map: {'client_name': 'ndyykbwdxur', 'r': 'y', 'top': 'ciw', 'order_number': 'wjdonbpitghk', 'sum': 'tcv', 'o': 'x', 'getdate': 'ufbdnsy'}\n"
     ]
    }
   ],
   "source": [
    "# Define the list of words to mask\n",
    "words_to_mask = ['client_name', 'r', 'top', 'order_number', 'sum', 'o', 'getdate']\n",
    "\n",
    "# Define the SQL string to mask\n",
    "query = \"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id\"\n",
    "\n",
    "# Mask the SQL string\n",
    "masked_sql_string, word_map = masking(words_to_mask, query)\n",
    "\n",
    "# Print the masked SQL string and the word map\n",
    "print(\"Masked SQL string:\", masked_sql_string)\n",
    "print(\"Word map:\", word_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_number', 'revenue', 'order_id', 'revenue_order', 'orders', 'client_name']\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "\n",
    "def get_identifiers(sql):\n",
    "    parsed_tokens = sqlparse.parse(sql)[0]\n",
    "    identifier_set = set()\n",
    "\n",
    "    reserved_words = ['TOP', 'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY', 'HAVING', 'ORDER', 'ASC', 'DESC']\n",
    "\n",
    "    def process_identifier(token, identifier_set):\n",
    "        identifier_name = token.get_real_name()\n",
    "        if '.' in identifier_name:\n",
    "            identifier_name = identifier_name.split('.')[1]\n",
    "        identifier_set.add(identifier_name)\n",
    "\n",
    "    def process_function_arguments(token, identifier_set):\n",
    "        if isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Parenthesis):\n",
    "            for subtoken in token.tokens:\n",
    "                process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def add_identifiers_from_function(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def process_where(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            if isinstance(subtoken, sqlparse.sql.Comparison):\n",
    "                process_identifier(subtoken.left, identifier_set)\n",
    "            elif isinstance(subtoken, sqlparse.sql.Identifier):\n",
    "                process_identifier(subtoken, identifier_set)\n",
    "\n",
    "    for token in parsed_tokens.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Function):\n",
    "            add_identifiers_from_function(token, identifier_set)\n",
    "            continue\n",
    "        if token.value.upper() in reserved_words:\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            process_identifier(token.left, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Where):\n",
    "            process_where(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "    return list(identifier_set)\n",
    "\n",
    "query = \"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_order in ('3245', '34244',  '4532')\"\n",
    "identifiers = get_identifiers(query)\n",
    "print(identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'identifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# Return the masked SQL string and the word map\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m sql, word_map\n\u001b[0;32m---> 26\u001b[0m masked_sql_wordmap \u001b[39m=\u001b[39m masking(identifiers, query)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(masked_sql_wordmap)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Still missing the Where cluse pick up - I am missing revenue_order field. This needs to be added to identifiers function. Here I am then this is ready. \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'identifiers' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def masking(identifiers, sql):\n",
    "    \"\"\"\n",
    "    This function takes in a list of identifiers and an SQL query as input, and replaces the identifiers in the SQL query with random words.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the mapping between original identifiers and masked words\n",
    "    word_map = {}\n",
    "    \n",
    "    # Loop through each identifier in the list of identifiers\n",
    "    for identifier in identifiers:\n",
    "        # Generate a random word to replace the original identifier\n",
    "        random_word = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=len(identifier)))\n",
    "        \n",
    "        # Add the mapping to the dictionary\n",
    "        word_map[identifier] = random_word\n",
    "        \n",
    "        # Replace the original identifier with the random word in the SQL string\n",
    "        sql = re.sub(r'\\b{}\\b'.format(identifier), random_word, sql)\n",
    "    \n",
    "    # Return the masked SQL string and the word map\n",
    "    return sql, word_map\n",
    "\n",
    "\n",
    "masked_sql_wordmap = masking(identifiers, query)\n",
    "print(masked_sql_wordmap)\n",
    "\n",
    "# Still missing the Where cluse pick up - I am missing revenue_order field. This needs to be added to identifiers function. Here I am then this is ready. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifiers(sql):\n",
    "    parsed_tokens = sqlparse.parse(sql)[0]\n",
    "    identifier_set = set()\n",
    "\n",
    "    reserved_words = ['TOP', 'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY', 'HAVING', 'ORDER', 'ASC', 'DESC']\n",
    "\n",
    "    def process_identifier(token, identifier_set):\n",
    "        identifier_name = token.get_real_name()\n",
    "        if '.' in identifier_name:\n",
    "            identifier_name = identifier_name.split('.')[1]\n",
    "        identifier_set.add(identifier_name)\n",
    "\n",
    "    def process_function_arguments(token, identifier_set):\n",
    "        if isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Parenthesis):\n",
    "            for subtoken in token.tokens:\n",
    "                process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def add_identifiers_from_function(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def process_where(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            if isinstance(subtoken, sqlparse.sql.Comparison):\n",
    "                process_identifier(subtoken.left, identifier_set)\n",
    "            elif isinstance(subtoken, sqlparse.sql.Identifier):\n",
    "                process_identifier(subtoken, identifier_set)\n",
    "\n",
    "    for token in parsed_tokens.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Function):\n",
    "            add_identifiers_from_function(token, identifier_set)\n",
    "            continue\n",
    "        if token.value.upper() in reserved_words:\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            process_identifier(token.left, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Where):\n",
    "            process_where(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "    return list(identifier_set)\n",
    "\n",
    "def sql_masking(identifiers, sql):\n",
    "    \"\"\"\n",
    "    This function takes in a list of identifiers and an SQL query as input, and replaces the identifiers in the SQL query with random words.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the mapping between original identifiers and masked words\n",
    "    word_map = {}\n",
    "    \n",
    "    # Loop through each identifier in the list of identifiers\n",
    "    for identifier in identifiers:\n",
    "        # Generate a random word to replace the original identifier\n",
    "        random_word = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=len(identifier)))\n",
    "        \n",
    "        # Add the mapping to the dictionary\n",
    "        word_map[identifier] = random_word\n",
    "        \n",
    "        # Replace the original identifier with the random word in the SQL string\n",
    "        sql = re.sub(r'\\b{}\\b'.format(identifier), random_word, sql)\n",
    "    \n",
    "    # Return the masked SQL string and the word map\n",
    "    return sql, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_number in ('2345', '9908', '6671')\"\n",
    "sql2=\"\"\"select K.a,K.b from (select H.b from (select G.c from (select F.d from (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders', 'order_number', 'client_name', 'revenue_number', 'order_id', 'revenue']\n"
     ]
    }
   ],
   "source": [
    "list_of_fields = get_identifiers(sql)\n",
    "print(list_of_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select top 10 getdate(), ffjoiuesdgyx, fjcgirovzue, sum(net_revenue) from bjrpjh o join akplyhv r on o.stlsxbqe = r.stlsxbqe where zrprazzgybxcri in ('2345', '9908', '6671')\n"
     ]
    }
   ],
   "source": [
    "masked_sql, word_map  = sql_masking(list_of_fields, sql)\n",
    "print(masked_sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demask fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demasking(word_map, masked_sql):\n",
    "    \"\"\"\n",
    "    This function takes in a word map and a masked SQL string as input and replaces the masked words with their original words.\n",
    "    \"\"\"\n",
    "    # Loop through each key-value pair in the word map\n",
    "    for original_word, masked_word in word_map.items():\n",
    "        # Replace the masked word with the original word in the SQL string\n",
    "        sql_string = re.sub(r'\\b{}\\b'.format(masked_word), original_word, sql)\n",
    "    \n",
    "    # Return the demasked SQL string\n",
    "    return sql_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demasked SQL string: select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_number in ('2345', '9908', '6671')\n"
     ]
    }
   ],
   "source": [
    "# Demask the SQL string\n",
    "demasked_sql = demasking(word_map, masked_sql)\n",
    "\n",
    "# Print the demasked SQL string\n",
    "print(\"Demasked SQL string:\", demasked_sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# loads .env file located in the current directory\n",
    "load_dotenv() \n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"select vid, ctextid, createdby, Created, description, p.dCode, dense_rank() over (partition by vid,ctextid order by Created) as rn from cte3 cross apply openjson(info) with (dCode varchar(30) '$.dCode',Description varchar(30) '$.description') p), R1 as (select * from Q1 where rn = 1), R2 as (select * from Q1 where rn = 2), Q2 as (select coalesce(R1.vid, R2.vid) as vid, coalesce(R1.ctextid, R2.ctextid) as ctextid, R1.Description as Description1, R1.dCode as dCode1, R2.Description as Description2, R2.dCode as dCode2 from R1 full outer join R2 on R2.vid = R1.vid and R2.ctextid = R1.ctextid and R2.[Description] = R1.[Description]) select vid, ctextid, (select top(1) createdby from R1 where vid = t.vid and ctextid = t.ctextid) as codername, coalesce( string_agg(case when dCode1 <> dCode2 then dCode1 end, ', '), '') as correctedcode, coalesce( string_agg(case when dCode2 is null then dCode1 end, ', '), '') as deletedcode, coalesce( string_agg(case when dCode1 is null then dCode2 end, ', '), '') as addedcode from Q2 as t group by vid, ctextid order by vid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the query string\n",
    "query_two = \"select h.iixctrvjg, h.uhczvi, h.mzxxvnfxrwmhdv, case when h.afprnxna is null then cast('06/09/1965' as date) else h.afprnxna end as afprnxna, h.khrcb, h.rmiuotcyosovtexo, h.cmvnonskhggg, h.ymrkz, h.rxfbop, h.jumlejbrvjeh, (select ct.number from NexusSuite.dbo.ClaimTransactions ct with (nolock) where ct.IsDeleted=0 and ct.[Key]=h.mzxxvnfxrwmhdv) as llmhiebazfms, case when NOT ( select top 1 cp.PolicyTransactionKey from NexusSuite.dbo.ClaimPolicies cp with (nolock) where cp.IsDeleted = 0 and cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv order by cp.[Key] desc ) is null then ( select top 1 pt.EffectiveDate from NexusSuite.dbo.PolicyTransactions pt with (nolock) where pt.IsDeleted = 0 and pt.[Key] = ( select top 1 cp.PolicyTransactionKey from NexusSuite.dbo.ClaimPolicies cp with (nolock) where cp.IsDeleted = 0 and cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv order by cp.[Key] desc ) ) else ( select top 1 cp.PolicyDate from NexusSuite.dbo.ClaimPolicies cp with (nolock) where cp.IsDeleted = 0 and cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv order by cp.[Key] desc ) end as cgumdtceaen, case when h.iixctrvjg = 'Loss Payments' then h.amount else 0 end bofavqlampcvx, case when h.iixctrvjg = 'Expense Payments' then h.amount else 0 end uputumbtmqjyolbw, case when h.iixctrvjg = 'Deductible Recoveries' then h.amount else 0 end ntcgwapznskrqtcvoghrg, case when h.iixctrvjg = 'Recoveries' then h.amount else 0 end jgvukysogr, case when h.iixctrvjg = 'Loss Reserves' then h.amount else 0 end iesyymzvcswej, case when h.iixctrvjg = 'Expense Reserves' then h.amount else 0 end rckwydvepayhvgnp, case when h.iixctrvjg = 'Prior Loss Reserves' then h.amount else 0 end nkkrbhcwchzqmttquzo, case when h.iixctrvjg = 'Prior Expense Reserves' then h.amount else 0 end lwaffwwvaegimvhgmtrkcc from ( select 'Loss Payments' as iixctrvjg, cp.[Key] as uhczvi, ct.[Key] as mzxxvnfxrwmhdv, ct.afprnxna, cp.khrcb, cp.rmiuotcyosovtexo, cp.Description as cmvnonskhggg, cp.ymrkz, cp.rxfbop, cp.Date as jumlejbrvjeh from NexusSuite.dbo.ClaimPayments cp with (nolock) inner join NexusSuite.dbo.ClaimCashTypes cct with (nolock) on cct.[Key] = cp.rmiuotcyosovtexo inner join NexusSuite.dbo.ClaimTransactions ct with (nolock)  on ct.[Key] = cp.mzxxvnfxrwmhdv where cp.IsDeleted = 0 and {% condition accounting_scope %} cp.khrcb {% endcondition %} and ( ( cp.khrcb = 'Underwriter' and  cct.ClaimAccountingClass = 'Loss') or ( cp.khrcb = 'Agent' and cct.ClaimAccountingClass in ( 'Loss','Deductible') ) ) --and cp.[Date] between @ReportStart and @ReportEnd and {% condition report_date %} cp.[Date] {% endcondition %} UNION ALL select 'Expense Payments' as iixctrvjg, cp.[Key] as uhczvi, ct.[Key] as mzxxvnfxrwmhdv, ct.afprnxna, cp.khrcb, cp.rmiuotcyosovtexo, cp.Description as cmvnonskhggg, cp.ymrkz, cp.rxfbop, cp.Date as jumlejbrvjeh from NexusSuite.dbo.ClaimPayments cp with (nolock) inner join NexusSuite.dbo.ClaimCashTypes cct with (nolock) on cct.[Key] = cp.rmiuotcyosovtexo inner join NexusSuite.dbo.ClaimTransactions ct with (nolock)  on ct.[Key] = cp.mzxxvnfxrwmhdv where cp.IsDeleted = 0 and {% condition accounting_scope %} cp.khrcb {% endcondition %} and cct.ClaimAccountingClass = 'Expense' and {% condition report_date %} cp.[Date] {% endcondition %} h\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sql=\"Snowflake\"\n",
    "from_sql=\"Transact-SQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_dialectify(to_sql, masked_sql):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": 'You are an expert SQL developer that is proficient in Transact-SQL, MySQL, PL/\\SQL, PL/\\pgSQL, SQLite, Snowflake SQL dialects.'},\n",
    "            {\"role\": \"system\", \"content\": 'Only return the converted sql code and do not explain the conversion process.'},\n",
    "            {\"role\": \"system\", \"content\": 'Check for the correctness of the entered SQL code. And make updates if necessary. List the changes succinctly in the chat.'},\n",
    "            {\"role\": \"system\", \"content\": 'Let''s think step by step.'},\n",
    "            {\"role\": \"user\", \"content\": f'Detect the dialect of the following SQL code: \"{masked_sql}\"'},\n",
    "            {\"role\": \"system\", \"content\": f'Check and fix errors for the top common SQL syntax mistakes for the detected dialect. List updated parts of the following SQL code: \"{masked_sql}\"'},\n",
    "            {\"role\": \"user\", \"content\": f'Convert the updated SQL code from detected dialect to \"{to_sql}\": \"\\n\\n{masked_sql}\"'}\n",
    "        ]\n",
    "    )\n",
    "    converted_sql = completion.choices[0].message.content\n",
    "    return converted_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ARE a few changes I need TO make TO the SQL code TO CONVERT it TO the Snowflake dialect: 1.\n",
      "REPLACE the USAGE OF `{% condition %}` WITH the equivalent Snowflake syntax. 2. Remove the `with (nolock)` statement. 3.\n",
      "REPLACE the USE OF `top 1` WITH the Snowflake `LIMIT 1` clause. 4. Remove the `between @ReportStart and @ReportEnd` clause. Here's the updated SQL code\n",
      "FOR Snowflake: ```\n",
      "SELECT h.iixctrvjg,\n",
      "       h.uhczvi,\n",
      "       h.mzxxvnfxrwmhdv,\n",
      "       CASE\n",
      "           WHEN h.afprnxna IS NULL THEN DATE_FROM_PARTS(1965, 06, 09)\n",
      "           ELSE h.afprnxna\n",
      "       END AS afprnxna,\n",
      "       h.khrcb,\n",
      "       h.rmiuotcyosovtexo,\n",
      "       h.cmvnonskhggg,\n",
      "       h.ymrkz,\n",
      "       h.rxfbop,\n",
      "       h.jumlejbrvjeh,\n",
      "\n",
      "  (SELECT ct.number\n",
      "   FROM NexusSuite.dbo.ClaimTransactions ct\n",
      "   WHERE ct.IsDeleted = 0\n",
      "     AND ct.\"Key\" = h.mzxxvnfxrwmhdv\n",
      "   LIMIT 1) AS llmhiebazfms,\n",
      "       CASE WHEN\n",
      "  (SELECT cp.PolicyTransactionKey\n",
      "   FROM NexusSuite.dbo.ClaimPolicies cp\n",
      "   WHERE cp.IsDeleted = 0\n",
      "     AND cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv\n",
      "   ORDER BY cp.\"Key\" DESC\n",
      "   LIMIT 1) IS NOT NULL THEN (\n",
      "SELECT pt.EffectiveDate\n",
      "FROM NexusSuite.dbo.PolicyTransactions pt\n",
      "WHERE pt.IsDeleted = 0\n",
      "  AND pt.\"Key\" = (\n",
      "  SELECT cp.PolicyTransactionKey\n",
      "  FROM NexusSuite.dbo.ClaimPolicies cp WHERE cp.IsDeleted = 0\n",
      "  AND cp.mzxxvnfx\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "# print(sql_dialectify(to_sql, query))\n",
    "print(sqlparse.format(sql_dialectify(to_sql, query_two), reindent=True, keyword_case='upper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Microsoft SQL Server specific syntax. In MySQL, we have to use LIMIT instead of TOP. Also, we don't have GETDATE() function in MySQL. Try the following code instead:\n",
      "\n",
      "```\n",
      "SELECT NOW(), blkyovxmofvp, imfutirkbqw, SUM(net_revenue)\n",
      "FROM ciwvio o \n",
      "JOIN rmegrla r ON o.zicjjvre = r.zicjjvre \n",
      "WHERE ejammpwncomqtc IN ('2345', '9908', '6671')\n",
      "LIMIT 10;\n",
      "```\n",
      "\n",
      "Note that `NOW()` returns the current date and time in MySQL. If you only want the current date, you can use `CURDATE()` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "masked_converted_sql = sql_dialectify(to_sql, masked_sql)\n",
    "# Display the converted SQL code\n",
    "print(masked_converted_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_dialectify_two(from_sql, to_sql, query):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": 'You are an expert SQL developer proficient in Transact-SQL, MySQL, PL/SQL, PL/pgSQL, SQLite, and Snowflake SQL dialects, with a focus on ensuring high accuracy during conversion.'},\n",
    "            {\"role\": \"system\", \"content\": 'Your task is to convert a specific SQL script from one dialect to another while maintaining the functionality and integrity of the original script.'},\n",
    "            {\"role\": \"system\", \"content\": f'The source SQL dialect is \"{from_sql}\", and the target SQL dialect is \"{to_sql}\". Your goal is to perform a precise conversion while addressing any incompatibilities or differences.'},\n",
    "            {\"role\": \"system\", \"content\": f'You will identify and address differences in data types and functions between the {from_sql} and {to_sql} dialects. For data types or functions without a direct equivalent, choose the most suitable alternative and include a comment in the converted SQL query explaining the change.'},\n",
    "            {\"role\": \"user\", \"content\": f'Please convert the following SQL code from \"{from_sql}\" to \"{to_sql}\" while ensuring the highest level of accuracy in maintaining the original functionality: \"\\n\\n{query}\"'},\n",
    "        ]\n",
    "    )\n",
    "    converted_sql = completion.choices[0].message.content\n",
    "    return converted_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the converted SQL code from Transact-SQL to Snowflake:\n",
      "\n",
      "```\n",
      "SELECT h.iixctrvjg, h.uhczvi, h.mzxxvnfxrwmhdv,\n",
      "       COALESCE(h.afprnxna, TO_DATE('06/09/1965', 'MM/DD/YYYY')) AS afprnxna,\n",
      "       h.khrcb, h.rmiuotcyosovtexo, h.cmvnonskhggg, h.ymrkz, h.rxfbop, h.jumlejbrvjeh,\n",
      "       (SELECT ct.number\n",
      "        FROM NexusSuite.dbo.ClaimTransactions ct\n",
      "        WHERE ct.IsDeleted = 0\n",
      "          AND ct.\"Key\" = h.mzxxvnfxrwmhdv) AS llmhiebazfms,\n",
      "\n",
      "       CASE WHEN (SELECT cp.PolicyTransactionKey\n",
      "                   FROM NexusSuite.dbo.ClaimPolicies cp\n",
      "                   WHERE cp.IsDeleted = 0\n",
      "                     AND cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv\n",
      "                   ORDER BY cp.\"Key\" DESC\n",
      "                   LIMIT 1) IS NOT NULL\n",
      "           THEN (SELECT pt.EffectiveDate\n",
      "                 FROM NexusSuite.dbo.PolicyTransactions pt\n",
      "                 WHERE pt.IsDeleted = 0\n",
      "                   AND pt.\"Key\" = (SELECT cp.PolicyTransactionKey\n",
      "                                   FROM NexusSuite.dbo.ClaimPolicies cp\n",
      "                                   WHERE cp.IsDeleted = 0\n",
      "                                     AND cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv\n",
      "                                   ORDER BY cp.\"Key\" DESC\n",
      "                                   LIMIT 1))\n",
      "           ELSE (SELECT cp.PolicyDate\n",
      "                 FROM NexusSuite.dbo.ClaimPolicies cp\n",
      "                 WHERE cp.IsDeleted = 0\n",
      "                   AND cp.mzxxvnfxrwmhdv = h.mzxxvnfxrwmhdv\n",
      "                 ORDER BY cp.\"Key\" DESC\n",
      "                 LIMIT 1)\n",
      "       END AS cgumdtceaen,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Loss Payments'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END bofavqlampcvx,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Expense Payments'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END uputumbtmqjyolbw,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Deductible Recoveries'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END ntcgwapznskrqtcvoghrg,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Recoveries'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END jgvukysogr,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Loss Reserves'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END iesyymzvcswej,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = 'Expense Reserves'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END rckwydvepayhvgnp,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = '\n",
      "PRIOR Loss Reserves'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END nkkrbhcwchzqmttquzo,\n",
      "\n",
      "       CASE WHEN h.iixctrvjg = '\n",
      "PRIOR Expense Reserves'\n",
      "           THEN h.amount\n",
      "           ELSE 0\n",
      "       END lwaffwwvaegimvhgmtrkcc\n",
      "FROM\n",
      "  (SELECT 'Loss Payments' AS iixctrvjg,\n",
      "                        cp.\"Key\" AS uhczvi,\n",
      "                        ct.\"Key\" AS mzxxvnfxrwmhdv,\n",
      "                        ct.afprnxna, ...\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "# print(sql_dialectify(from_sql, to_sql, query))\n",
    "print(sqlparse.format(sql_dialectify_two(from_sql, to_sql, query_two), reindent=True, keyword_case='upper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"select r.transaction_key,\n",
    "    isnull(sum(r.net_premium),0) as net_premium,\n",
    "    isnull(sum(r.payment_amount),0) as payment_amount,\n",
    "    isnull(sum(r.net_premium),0) - isnull(sum(r.payment_amount),0) as balance_due\n",
    "    from (\n",
    "    select\n",
    "    p.TransactionKey as transaction_key,\n",
    "    isnull(sum(p.NetPremium),0) as net_premium,\n",
    "    0 as payment_amount\n",
    "    from NexusSuite.dbo.policies p with (nolock)\n",
    "    INNER JOIN NexusSuite.dbo.policytransactions pt with (nolock) on pt.[Key] = p.TransactionKey\n",
    "    INNER JOIN NexusSuite.dbo.companies c with (nolock) on c.[Key] = pt.CompanyKey and c.CompanyType in (0,1,2,3)\n",
    "    where\n",
    "    p.IsDeleted = 0 and p.IsVoided = 0 and pt.IsDeleted=0 and c.[key] > 1\n",
    "     and p.CreatedDate between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime)\n",
    "     group by p.TransactionKey\n",
    "\n",
    "    union all\n",
    "\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    --bof: get endorsements\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    select\n",
    "    e.TransactionKey,\n",
    "    isnull(sum(e.NetPremium),0) as NetPremium,\n",
    "    0 as payment_amount\n",
    "    from NexusSuite.dbo.endorsements e with (nolock)\n",
    "    INNER JOIN NexusSuite.dbo.policytransactions pt with (nolock) on pt.[Key] = e.TransactionKey\n",
    "    INNER JOIN NexusSuite.dbo.companies c with (nolock) on c.[Key] = pt.CompanyKey and c.CompanyType in (0,1,2,3)\n",
    "    where e.IsDeleted = 0 and e.IsVoided = 0 and pt.IsDeleted=0 and c.[key] > 1\n",
    "    and e.CreatedDate between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime)\n",
    "    group by e.TransactionKey\n",
    "\n",
    "    union all\n",
    "\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    --bof: get cpls\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    select\n",
    "    c.TransactionKey,\n",
    "    isnull(sum(c.NetPremium),0) as NetPremium,\n",
    "    0 as payment_amount\n",
    "    from NexusSuite.dbo.cpls c with (nolock)\n",
    "    INNER JOIN NexusSuite.dbo.policytransactions pt with (nolock) on pt.[Key] = c.TransactionKey\n",
    "    INNER JOIN NexusSuite.dbo.companies co with (nolock) on co.[Key] = pt.CompanyKey and co.CompanyType in (0,1,2,3)\n",
    "    where\n",
    "    c.IsDeleted = 0 and c.IsVoided = 0 and pt.IsDeleted=0 and co.[key] > 1\n",
    "    and c.CreatedDate between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime)\n",
    "    group by c.TransactionKey\n",
    "\n",
    "    union all\n",
    "\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    --bof: get adjustments\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    select\n",
    "    a.TransactionKey,\n",
    "    isnull(sum(a.NetPremium),0) as NetPremium,\n",
    "    0 as payment_amount\n",
    "    from NexusSuite.dbo.Adjustments a with (nolock)\n",
    "    INNER JOIN NexusSuite.dbo.policytransactions pt with (nolock) on pt.[Key] = a.TransactionKey\n",
    "    INNER JOIN NexusSuite.dbo.companies c with (nolock) on c.[Key] = pt.CompanyKey and c.CompanyType in (0,1,2,3)\n",
    "    where a.IsDeleted = 0 and pt.IsDeleted=0 and c.[key] > 1\n",
    "    and a.[CreatedDate] between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime)\n",
    "    group by a.TransactionKey\n",
    "\n",
    "    union all\n",
    "    select\n",
    "    f.TransactionKey, isnull(sum(f.Amount),0) as NetPremium, 0 as payment_amount from NexusSuite.dbo.fees f with (nolock) INNER join NexusSuite.dbo.PolicyTransactions pt with (nolock) on pt.[key] = f.TransactionKey INNER JOIN NexusSuite.dbo.companies c with (nolock) on c.[Key] = pt.CompanyKey and c.CompanyType in (0,1,2,3) where f.IsDeleted = 0 and pt.IsDeleted=0 and c.[key] > 1 and f.CreatedDate between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime) group by f.TransactionKey union all select re.TransactionKey, isnull(sum(re.AgentReceivable),0) as NetPremium, 0 as payment_amountfrom NexusSuite.dbo.Reinsurance re with (nolock) INNER JOIN NexusSuite.dbo.policytransactions pt with (nolock) on pt.[Key] = re.TransactionKey INNER JOIN NexusSuite.dbo.companies c with (nolock) on c.[Key] = pt.CompanyKey and c.CompanyType in (0,1,2,3) where re.IsDeleted = 0 and pt.IsDeleted=0 and c.[key] > 1 \n",
    "    and re.CreatedDate between cast('01/01/1900' as datetime) and cast(getdate()-1 as datetime) group by re.TransactionKey\n",
    "    \"\"\"\n",
    "\n",
    "to_sql=\"Snowflake-cloud-data-platform\"\n",
    "from_sql=\"Transact-SQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_dialectify(from_sql, to_sql, original_sql, mask_fields=False, identifiers=None):\n",
    "    # If mask_fields is True, mask the fields using the provided sql_masking function\n",
    "    if mask_fields and identifiers:\n",
    "        masked_sql, word_map = sql_masking(identifiers, original_sql)\n",
    "    else:\n",
    "        masked_sql = original_sql\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        #model=\"gpt-4\",\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "           {\"role\": \"system\", \"content\": 'Act as CODEX (\"COding DEsign eXpert\"), an expert coder with proficiency in SQL programming language.'},\n",
    "            {\"role\": \"system\", \"content\": 'You are proficient in Transact-SQL, MySQL, PL/SQL, PL/pgSQL, SQLite, and Snowflake SQL dialects, with a focus on high accuracy dialect to dialect conversions.'},\n",
    "            {\"role\": \"system\", \"content\": 'Your task is to convert a specific SQL script from one SQL dialect to another SQL dialect while maintaining the functionality and integrity of the original script.'},\n",
    "            {\"role\": \"system\", \"content\": f'The source SQL dialect is \"{from_sql}\", and the target SQL dialect is \"{to_sql}\". Your goal is to perform a precise SQL dialect conversion while addressing any incompatibilities or differences.'},\n",
    "            {\"role\": \"system\", \"content\": 'Always follow the coding best practices by writing clean, modular code with proper security measures and leveraging design patterns.'},\n",
    "            {\"role\": \"system\", \"content\": f'Let''s think step by step. First, you will identify the differences between the {from_sql} and {to_sql} dialects. Then, you will convert the SQL code from {from_sql} to {to_sql} while ensuring the highest level of accuracy in maintaining the original functionality.'},\n",
    "            {\"role\": \"system\", \"content\": f'You will identify and address differences in data types and functions between the {from_sql} and {to_sql} dialects. For data types or functions without a direct equivalent, choose the most suitable alternative'},\n",
    "            {\"role\": \"system\", \"content\": 'You will return your answers in two sections. In the first section you will return the converted sql query in a code block and you will add a semi colon (;) at the end of the query if it was not inputted. You will title this section as \"\\n # Converted SQL: \".'},\n",
    "            {\"role\": \"system\", \"content\": 'In the second section you will return any comments and the explanations of the changes in bulled points. You will title this section as \"\\n List of changes: \".'},\n",
    "            {\"role\": \"user\", \"content\": f'Convert the following SQL code from \"{from_sql}\" to \"{to_sql}\" while ensuring the highest level of accuracy in maintaining the original functionality: ```\\n\\n{masked_sql}```'},\n",
    "        \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    converted_sql = completion.choices[0].message.content\n",
    "\n",
    "    # If mask_fields is True, replace the masked words with the original identifiers\n",
    "    if mask_fields and identifiers:\n",
    "        for original, masked in word_map.items():\n",
    "            converted_sql = converted_sql.replace(masked, original)\n",
    "\n",
    "    return converted_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Converted SQL:\n",
      " ```\n",
      "select vid, ctextid, createdby, Created, description, p.dCode, dense_rank() over (partition by vid,ctextid order by Created) as rn from cte3, lateral flatten(input => parse_json(info), path => 'p') p, R1 as (select * from Q1 where rn = 1), R2 as (select * from Q1 where rn = 2), Q2 as (select coalesce(R1.vid, R2.vid) as vid, coalesce(R1.ctextid, R2.ctextid) as ctextid, R1.Description as Description1, R1.dCode as dCode1, R2.Description as Description2, R2.dCode as dCode2 from R1 full outer join R2 on R2.vid = R1.vid and R2.ctextid = R1.ctextid and R2.\"Description\" = R1.\"Description\") select vid, ctextid, (select top(1) createdby from R1 where vid = t.vid and ctextid = t.ctextid) as codername, coalesce( listagg(case when dCode1 <> dCode2 then dCode1 end, ', ') within group (order by dCode1), '') as correctedcode, coalesce( listagg(case when dCode2 is null then dCode1 end, ', ') within group (order by dCode1), '') as deletedcode, coalesce( listagg(case when dCode1 is null then dCode2 end, ', ') within group (order by dCode2), '') as addedcode from Q2 as t group by vid, ctextid order by vid;\n",
      "``` List OF changes: - Replaced \"cross apply\" WITH \"lateral flatten\". - Replaced \"coalesce(R2.[Description], R1.[Description])\" WITH \"coalesce(R2.\"Description\", R1.\"Description\")\". - Replaced \"string_agg\" WITH \"listagg\". - Replaced \"top(1)\" WITH \"limit 1\".\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "# print(sql_dialectify(from_sql, to_sql, query))\n",
    "text = sqlparse.format(sql_dialectify(from_sql, to_sql, query), reindent=True, keyword_case='upper')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Converted SQL: ```\n",
      "SELECT r.transaction_key,\n",
      "    COALESCE(SUM(r.net_premium), 0) AS net_premium,\n",
      "    COALESCE(SUM(r.payment_amount), 0) AS payment_amount,\n",
      "    COALESCE(SUM(r.net_premium), 0) - COALESCE(SUM(r.payment_amount), 0) AS balance_due\n",
      "FROM (\n",
      "    SELECT\n",
      "        p.TransactionKey AS transaction_key,\n",
      "        COALESCE(SUM(p.NetPremium), 0) AS net_premium,\n",
      "        0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.policies p\n",
      "    INNER JOIN NexusSuite.dbo.policytransactions pt ON pt.[Key] = p.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies c ON c.[Key] = pt.CompanyKey AND c.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE\n",
      "        p.IsDeleted = 0 AND p.IsVoided = 0 AND pt.IsDeleted = 0 AND c.[key] > 1\n",
      "        AND p.CreatedDate BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY p.TransactionKey\n",
      "\n",
      "    UNION ALL\n",
      "\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    --bof: get endorsements\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    SELECT\n",
      "        e.TransactionKey,\n",
      "        COALESCE(SUM(e.NetPremium), 0) AS NetPremium,\n",
      "        0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.endorsements e\n",
      "    INNER JOIN NexusSuite.dbo.policytransactions pt ON pt.[Key] = e.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies c ON c.[Key] = pt.CompanyKey AND c.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE e.IsDeleted = 0 AND e.IsVoided = 0 AND pt.IsDeleted = 0 AND c.[key] > 1\n",
      "        AND e.CreatedDate BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY e.TransactionKey\n",
      "\n",
      "    UNION ALL\n",
      "\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    --bof: get cpls\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    SELECT\n",
      "        c.TransactionKey,\n",
      "        COALESCE(SUM(c.NetPremium), 0) AS NetPremium,\n",
      "        0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.cpls c\n",
      "    INNER JOIN NexusSuite.dbo.policytransactions pt ON pt.[Key] = c.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies co ON co.[Key] = pt.CompanyKey AND co.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE\n",
      "        c.IsDeleted = 0 AND c.IsVoided = 0 AND pt.IsDeleted = 0 AND co.[key] > 1\n",
      "        AND c.CreatedDate BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY c.TransactionKey\n",
      "\n",
      "    UNION ALL\n",
      "\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    --bof: get adjustments\n",
      "    --------------------------------------------------------------------------------------------------------------------\n",
      "    SELECT\n",
      "        a.TransactionKey,\n",
      "        COALESCE(SUM(a.NetPremium), 0) AS NetPremium,\n",
      "        0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.Adjustments a\n",
      "    INNER JOIN NexusSuite.dbo.policytransactions pt ON pt.[Key] = a.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies c ON c.[Key] = pt.CompanyKey AND c.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE a.IsDeleted = 0 AND pt.IsDeleted = 0 AND c.[key] > 1\n",
      "        AND a.[CreatedDate] BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY a.TransactionKey\n",
      "\n",
      "    UNION ALL\n",
      "    SELECT\n",
      "        f.TransactionKey, COALESCE(SUM(f.Amount), 0) AS NetPremium, 0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.fees f\n",
      "    INNER JOIN NexusSuite.dbo.PolicyTransactions pt ON pt.[key] = f.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies c ON c.[Key] = pt.CompanyKey AND c.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE f.IsDeleted = 0 AND pt.IsDeleted = 0 AND c.[key] > 1\n",
      "        AND f.CreatedDate BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY f.TransactionKey\n",
      "\n",
      "    UNION ALL\n",
      "\n",
      "    SELECT re.TransactionKey, COALESCE(SUM(re.AgentReceivable), 0) AS NetPremium, 0 AS payment_amount\n",
      "    FROM NexusSuite.dbo.Reinsurance re\n",
      "    INNER JOIN NexusSuite.dbo.policytransactions pt ON pt.[Key] = re.TransactionKey\n",
      "    INNER JOIN NexusSuite.dbo.companies c ON c.[Key] = pt.CompanyKey AND c.CompanyType IN (0, 1, 2, 3)\n",
      "    WHERE re.IsDeleted = 0 AND pt.IsDeleted = 0 AND c.[key] > 1\n",
      "        AND re.CreatedDate BETWEEN '1900-01-01'::DATE AND (CURRENT_DATE - INTERVAL '1 day')\n",
      "    GROUP BY re.TransactionKey\n",
      ") r\n",
      "GROUP BY r.transaction_key;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting and storing the text for each variable\n",
    "variable1_text = text.split(\"``` \")[0]\n",
    "\n",
    "variable2_text = text.split(\"``` \")[-1]\n",
    "\n",
    "#print(sqlparse.format(variable1_text, reindent=True, keyword_case='upper'))\n",
    "#print(variable2_text)\n",
    "print(variable1_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Details:  Replaced \"isnull\" WITH \"NVL\" FUNCTION AS Snowflake does NOT support \"isnull\" function.  Replaced \"cast\" FUNCTION WITH \"to_date\" FUNCTION AS Snowflake does NOT support \"cast\" function.  Replaced \"getdate()\" FUNCTION WITH \"current_date()\" FUNCTION AS Snowflake does NOT support \"getdate()\" function.  Replaced square brackets around COLUMN NAMES WITH DOUBLE quotes AS Snowflake does NOT support square brackets.  Removed \"(nolock)\"', 'FROM the query AS Snowflake does NOT support this syntax.  Added semicolon AT the END OF the query.']\n"
     ]
    }
   ],
   "source": [
    "def bullet_points_to_list(text):\n",
    "    lines = text.split('\\n')\n",
    "    items = [line.strip().replace('-', '').strip() for line in lines if line.strip()]\n",
    "    return items\n",
    "\n",
    "formatted_list = bullet_points_to_list(variable2_text)\n",
    "print(formatted_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add _view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SQL:\n",
      " \n",
      "    select K.a,K.b from (select H.b from (select G.c from (select F.d from\n",
      "    (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\n",
      "    \n",
      "Updated SQL:\n",
      " \n",
      "    select K_view.a_view,K_view.b_view from (select H.b_view from (select G.c from (select F.d from\n",
      "    (select E.e from A, B, C, D, E), F), G), H), I_view, J_view, K_view order b_viewy 1,2;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML, Name\n",
    "\n",
    "\n",
    "def extract_table_identifiers(token_stream):\n",
    "    for item in token_stream:\n",
    "        if isinstance(item, Identifier):\n",
    "            yield item.get_real_name()\n",
    "        elif isinstance(item, IdentifierList):\n",
    "            for identifier in item.get_identifiers():\n",
    "                if isinstance(identifier, Identifier):\n",
    "                    yield identifier.get_real_name()\n",
    "\n",
    "\n",
    "def append_view_to_tables(sql, tables):\n",
    "    updated_sql = sql\n",
    "    for table in tables:\n",
    "        updated_sql = updated_sql.replace(table, f\"{table}_view\")\n",
    "    return updated_sql\n",
    "\n",
    "\n",
    "def extract_tables(sql):\n",
    "    parsed = sqlparse.parse(sql)[0]\n",
    "    token_stream = extract_table_identifiers(parsed.tokens)\n",
    "    table_set = set()\n",
    "\n",
    "    for token in token_stream:\n",
    "        if token.upper() != \"AS\":\n",
    "            table_set.add(token)\n",
    "\n",
    "    return list(table_set)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sql = \"\"\"\n",
    "    select K.a,K.b from (select H.b from (select G.c from (select F.d from\n",
    "    (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\n",
    "    \"\"\"\n",
    "\n",
    "    tables = extract_tables(sql)\n",
    "    updated_sql = append_view_to_tables(sql, tables)\n",
    "\n",
    "    print('Original SQL:\\n', sql)\n",
    "    print('Updated SQL:\\n', updated_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Token' object has no attribute 'get_real_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      4\u001b[0m     sql \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m    select K.a,K.b from (select H.b from (select G.c from (select F.d from\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m    (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     tables \u001b[39m=\u001b[39m extract_tables(sql)\n\u001b[1;32m     10\u001b[0m     updated_sql \u001b[39m=\u001b[39m append_view_to_tables(sql, tables)\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTables:\u001b[39m\u001b[39m'\u001b[39m, tables)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mextract_tables\u001b[0;34m(sql)\u001b[0m\n\u001b[1;32m     26\u001b[0m token_stream \u001b[39m=\u001b[39m extract_table_identifiers(parsed\u001b[39m.\u001b[39mtokens)\n\u001b[1;32m     27\u001b[0m table_set \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m token_stream:\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mupper() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAS\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     31\u001b[0m         table_set\u001b[39m.\u001b[39madd(token)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mextract_table_identifiers\u001b[0;34m(token_stream)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, IdentifierList):\n\u001b[1;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m identifier \u001b[39min\u001b[39;00m item\u001b[39m.\u001b[39mget_identifiers():\n\u001b[0;32m---> 12\u001b[0m         \u001b[39myield\u001b[39;00m identifier\u001b[39m.\u001b[39;49mget_real_name()\n\u001b[1;32m     13\u001b[0m \u001b[39melif\u001b[39;00m item\u001b[39m.\u001b[39mttype \u001b[39mis\u001b[39;00m Name:\n\u001b[1;32m     14\u001b[0m     \u001b[39myield\u001b[39;00m item\u001b[39m.\u001b[39mvalue\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Token' object has no attribute 'get_real_name'"
     ]
    }
   ],
   "source": [
    "#sql=\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_number in ('2345', '9908', '6671')\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sql = \"\"\"\n",
    "    select K.a,K.b from (select H.b from (select G.c from (select F.d from\n",
    "    (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\n",
    "    \"\"\"\n",
    "\n",
    "    tables = extract_tables(sql)\n",
    "    updated_sql = append_view_to_tables(sql, tables)\n",
    "    print('Tables:', tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_code(input_string):\n",
    "    \"\"\"\n",
    "    This function takes in a string as input and returns two strings - text and code.\n",
    "    The code is the portion within triple quotes, and the text is the rest of the string.\n",
    "    \"\"\"\n",
    "    # Find the index of the first occurrence of triple quotes\n",
    "    start_index = input_string.find('\"\"\"')\n",
    "    \n",
    "    # If triple quotes are not found, return the entire string as text and an empty string as code\n",
    "    if start_index == -1:\n",
    "        return input_string.strip(), \"\"\n",
    "    \n",
    "    # Find the index of the second occurrence of triple quotes\n",
    "    end_index = input_string.find('\"\"\"', start_index + 3)\n",
    "    \n",
    "    # If the second occurrence of triple quotes is not found, return the entire string as text and an empty string as code\n",
    "    if end_index == -1:\n",
    "        return input_string.strip(), \"\"\n",
    "    \n",
    "    # Extract the text before the triple quotes and the code between the triple quotes\n",
    "    text = input_string[:start_index].strip()\n",
    "    code = input_string[start_index+3:end_index].strip()\n",
    "    \n",
    "    return text, code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "The SQL code cannot be directly converted to MySQL as it contains a few syntax errors specific to MS SQL Server. Here are the corrected SQL code to be used for the conversion:\n",
      "\n",
      "```mysql\n",
      "SELECT CAST(NOW() AS DATE), xzxpaynvykdy, zaoejvuuefv, SUM(net_revenue) \n",
      "FROM fhwdiv o \n",
      "JOIN zhmpjpo r \n",
      "ON o.tzmiorqu = r.tzmiorqu \n",
      "WHERE eazuotfoyhdfcx IN ('2345', '9908', '6671')\n",
      "LIMIT 10;\n",
      "```\n",
      "\n",
      "Code:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_string = \"The SQL code cannot be directly converted to MySQL as it contains a few syntax errors specific to MS SQL Server. Here are the corrected SQL code to be used for the conversion:\\n\\n```mysql\\nSELECT CAST(NOW() AS DATE), xzxpaynvykdy, zaoejvuuefv, SUM(net_revenue) \\nFROM fhwdiv o \\nJOIN zhmpjpo r \\nON o.tzmiorqu = r.tzmiorqu \\nWHERE eazuotfoyhdfcx IN ('2345', '9908', '6671')\\nLIMIT 10;\\n```\"\n",
    "\n",
    "text, code = split_text_and_code(input_string)\n",
    "\n",
    "print(\"Text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nCode:\")\n",
    "print(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select top 10 getdate(), aqqddmhwgxte, zfzxhmzsvdb, sum(net_revenue) from tckvql o join udghcyk r on o.devgbttq = r.devgbttq where edgygahbmavogp in ('2345', '9908', '6671')\n",
      "{'revenue': 'udghcyk', 'client_name': 'zfzxhmzsvdb', 'revenue_number': 'edgygahbmavogp', 'orders': 'tckvql', 'order_id': 'devgbttq', 'order_number': 'aqqddmhwgxte'}\n",
      "['revenue', 'client_name', 'revenue_number', 'orders', 'order_id', 'order_number']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import streamlit as st\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# loads .env file located in the current directory\n",
    "load_dotenv() \n",
    "\n",
    "import sqlparse\n",
    "import random\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML, Name\n",
    "to_sql = \"Snowflake\"\n",
    "sql=\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_number in ('2345', '9908', '6671')\"\n",
    "\n",
    "# Extract fields for masking - identifier_set\n",
    "\n",
    "def get_identifiers(sql):\n",
    "    parsed_tokens = sqlparse.parse(sql)[0]\n",
    "    identifier_set = set()\n",
    "\n",
    "    reserved_words = ['TOP', 'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY', 'HAVING', 'ORDER', 'ASC', 'DESC']\n",
    "\n",
    "    def process_identifier(token, identifier_set):\n",
    "        identifier_name = token.get_real_name()\n",
    "        if '.' in identifier_name:\n",
    "            identifier_name = identifier_name.split('.')[1]\n",
    "        identifier_set.add(identifier_name)\n",
    "\n",
    "    def process_function_arguments(token, identifier_set):\n",
    "        if isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Parenthesis):\n",
    "            for subtoken in token.tokens:\n",
    "                process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def add_identifiers_from_function(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            process_function_arguments(subtoken, identifier_set)\n",
    "\n",
    "    def process_where(token, identifier_set):\n",
    "        for subtoken in token.tokens:\n",
    "            if isinstance(subtoken, sqlparse.sql.Comparison):\n",
    "                process_identifier(subtoken.left, identifier_set)\n",
    "            elif isinstance(subtoken, sqlparse.sql.Identifier):\n",
    "                process_identifier(subtoken, identifier_set)\n",
    "\n",
    "    for token in parsed_tokens.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Function):\n",
    "            add_identifiers_from_function(token, identifier_set)\n",
    "            continue\n",
    "        if token.value.upper() in reserved_words:\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            process_identifier(token.left, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Where):\n",
    "            process_where(token, identifier_set)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token, identifier_set)\n",
    "    return list(identifier_set)\n",
    "\n",
    "def sql_masking(identifiers, sql):\n",
    "    \"\"\"\n",
    "    This function takes in a list of identifiers and an SQL query as input, and replaces the identifiers in the SQL query with random words.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the mapping between original identifiers and masked words\n",
    "    word_map = {}\n",
    "    \n",
    "    # Loop through each identifier in the list of identifiers\n",
    "    for identifier in identifiers:\n",
    "        # Generate a random word to replace the original identifier\n",
    "        random_word = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=len(identifier)))\n",
    "        \n",
    "        # Add the mapping to the dictionary\n",
    "        word_map[identifier] = random_word\n",
    "        \n",
    "        # Replace the original identifier with the random word in the SQL string\n",
    "        sql = re.sub(r'\\b{}\\b'.format(identifier), random_word, sql)\n",
    "    \n",
    "    # Return the masked SQL string and the word map\n",
    "    return sql, word_map\n",
    "\n",
    "def sql_dialectify(to_sql, masked_sql):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": 'You are an expert SQL developer that is proficient in MS SQL Server, MySQL, Oracle, PostgreSQL, SQLite, Snowflake SQL dialects.'},\n",
    "            {\"role\": \"system\", \"content\": 'Only return the converted sql code and do not explain the conversion process.'},\n",
    "            {\"role\": \"system\", \"content\": 'Check for the correctness of the entered SQL code. And make updates if necessary. List the changes succinctly in the chat.'},\n",
    "            {\"role\": \"system\", \"content\": 'Let''s think step by step.'},\n",
    "            {\"role\": \"user\", \"content\": f'Detect the dialect of the following SQL code: \"{masked_sql}\"'},\n",
    "            {\"role\": \"system\", \"content\": f'Check and fix errors for the top common SQL syntax mistakes for the detected dialect. List updated parts of the following SQL code: \"{masked_sql}\"'},\n",
    "            {\"role\": \"user\", \"content\": f'Convert the updated SQL code from detected dialect to \"{to_sql}\": \"\\n\\n{masked_sql}\"'}\n",
    "        ]\n",
    "    )\n",
    "    converted_sql = completion.choices[0].message.content\n",
    "    return converted_sql\n",
    "\n",
    "# Demask Converted SQL\n",
    "\n",
    "def demasking(word_map, masked_sql):\n",
    "    \"\"\"\n",
    "    This function takes in a word map and a masked SQL string as input and replaces the masked words with their original words.\n",
    "    \"\"\"\n",
    "    demasked_sql = masked_sql\n",
    "\n",
    "    # Loop through each key-value pair in the word map\n",
    "    for original_word, masked_word in word_map.items():\n",
    "        # Replace the masked word with the original word in the SQL string\n",
    "        demasked_sql = re.sub(r'\\b{}\\b'.format(masked_word), original_word, demasked_sql)\n",
    "    \n",
    "    # Return the demasked SQL string\n",
    "    return demasked_sql\n",
    "\n",
    "\n",
    "# Convert SQL dialect\n",
    "\n",
    "list_of_fields = get_identifiers(sql)\n",
    "masked_sql, word_map = sql_masking(list_of_fields, sql)\n",
    "print(masked_sql)\n",
    "print(word_map)\n",
    "print(list_of_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'revenue': 'udghcyk', 'client_name': 'zfzxhmzsvdb', 'revenue_number': 'edgygahbmavogp', 'orders': 'tckvql', 'order_id': 'devgbttq', 'order_number': 'aqqddmhwgxte'}\n",
      "select top 10 getdate(), aqqddmhwgxte, zfzxhmzsvdb, sum(net_revenue) from tckvql o join udghcyk r on o.devgbttq = r.devgbttq where edgygahbmavogp in ('2345', '9908', '6671')\n",
      "This query can be converted from MS SQL Server to Snowflake by changing \"TOP\" clause to \"LIMIT\" and removing \"getdate()\" function, as it is not supported in Snowflake. Here's the converted code:\n",
      "\n",
      "```\n",
      "SELECT aqqddmhwgxte, zfzxhmzsvdb, SUM(net_revenue)\n",
      "FROM tckvql o \n",
      "JOIN udghcyk r ON o.devgbttq = r.devgbttq \n",
      "WHERE edgygahbmavogp IN ('2345', '9908', '6671')\n",
      "LIMIT 10;\n",
      "```\n",
      "This query can be converted from MS SQL Server to Snowflake by changing \"TOP\" clause to \"LIMIT\" and removing \"getdate()\" function, as it is not supported in Snowflake. Here's the converted code:\n",
      "\n",
      "```\n",
      "SELECT order_number, client_name, SUM(net_revenue)\n",
      "FROM orders o \n",
      "JOIN revenue r ON o.order_id = r.order_id \n",
      "WHERE revenue_number IN ('2345', '9908', '6671')\n",
      "LIMIT 10;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "masked_converted_sql = sql_dialectify(to_sql, masked_sql)\n",
    "demasked_sql = demasking(word_map, masked_converted_sql)\n",
    "print(word_map)\n",
    "print(masked_sql)\n",
    "print(masked_converted_sql)\n",
    "print(demasked_sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Start - adding it all into a single function\n",
    "- Toggle to add _view to the returned results. Encode and decode has to be done (Run both find table and add _view functions in a single function together. The idea should be to find the tables and append _view at that time and not call them and append them back by word. I guess I can Mask them as I extract the words as well. That is probably why the masking is not working as intended. So one function to do many things…….."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"select top 10 getdate(), order_number, client_name, sum(net_revenue) from orders o join revenue r on o.order_id = r.order_id where revenue_number in ('2345', '9908', '6671')\"\n",
    "sql2=\"\"\"select K.a,K.b from (select H.b from (select G.c from (select F.d from (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import streamlit as st\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# loads .env file located in the current directory\n",
    "load_dotenv() \n",
    "\n",
    "import sqlparse\n",
    "import random\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML, Name\n",
    "\n",
    "import time\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "import re\n",
    "\n",
    "def parse_sql(sql):\n",
    "    \"\"\"\n",
    "    This function takes in an SQL query as input and returns a list of identifiers.\n",
    "    \"\"\"\n",
    "    def process_identifier(token):\n",
    "        identifier_name = token.get_real_name()\n",
    "        if '.' in identifier_name:\n",
    "            identifier_name = identifier_name.split('.')[1]\n",
    "        if not re.match(r\"(TOP|LIMIT)$\", identifier_name, re.IGNORECASE):\n",
    "            identifier_set.add(identifier_name)\n",
    "\n",
    "    def process_function_arguments(token):\n",
    "        if isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token)\n",
    "        elif isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier)\n",
    "        elif isinstance(token, sqlparse.sql.Parenthesis):\n",
    "            for subtoken in token.tokens:\n",
    "                process_function_arguments(subtoken)\n",
    "\n",
    "    def add_identifiers_from_function(token):\n",
    "        if not re.match(r\"^(COUNT|SUM|AVG|MIN|MAX|FIRST|LAST)$\", token.get_name(), re.IGNORECASE):\n",
    "            for subtoken in token.tokens:\n",
    "                process_function_arguments(subtoken)\n",
    "\n",
    "    def process_where(token):\n",
    "        for subtoken in token.tokens:\n",
    "            if isinstance(subtoken, sqlparse.sql.Comparison): \n",
    "                process_identifier(subtoken.left)         \n",
    "            elif isinstance(subtoken, sqlparse.sql.Identifier): \n",
    "                process_identifier(subtoken)\n",
    "\n",
    "# Parse the SQL query\n",
    "    parsed_tokens = sqlparse.parse(sql)[0]\n",
    "    identifier_set = set()\n",
    "\n",
    "    for token in parsed_tokens.tokens:\n",
    "        if isinstance(token, sqlparse.sql.Function): \n",
    "            add_identifiers_from_function(token)\n",
    "            continue\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if isinstance(identifier, sqlparse.sql.Identifier):\n",
    "                    process_identifier(identifier)\n",
    "        elif isinstance(token, sqlparse.sql.Comparison):\n",
    "            process_identifier(token.left)\n",
    "        elif isinstance(token, sqlparse.sql.Where):\n",
    "            process_where(token)\n",
    "        elif isinstance(token, sqlparse.sql.Identifier):\n",
    "            process_identifier(token)\n",
    "\n",
    "    return list(identifier_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orders',\n",
       " 'order_number',\n",
       " 'client_name',\n",
       " 'order_id',\n",
       " 'revenue_number',\n",
       " 'revenue']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_sql(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
